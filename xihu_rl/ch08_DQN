import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import logging
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from collections import deque
from typing import Tuple, List


# ==========================================
# 0. 工程化配置 (Configuration)
# ==========================================
class Config:
    """
    全局配置类：集中管理超参数和环境设定
    """
    # 环境设定
    GRID_SIZE = 4
    STATE_DIM = 16  # 4x4 Grid -> One-hot vector size 16
    ACTION_DIM = 4  # 0:Up, 1:Down, 2:Left, 3:Right

    # 关键修改：定义陷阱列表 [(row, col), ...]
    # 设计了一个这就如同“路障”的陷阱布局，迫使 Agent 绕路
    TRAPS = [(1, 1), (3, 2), (1, 3)]

    # 训练超参数
    LR = 0.005  # Learning Rate
    GAMMA = 0.9  # Discount Factor
    BATCH_SIZE = 64  # Mini-batch size
    TARGET_UPDATE_FREQ = 50  # Target Network 同步频率

    # 实验流程参数
    COLLECTION_STEPS = 6000  # Phase 1: 随机探索步数 (增加一点以覆盖复杂地形)
    TRAINING_ITERATIONS = 2500  # Phase 2: 训练迭代次数

    # 硬件设备自动检测
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# 配置日志格式
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# ==========================================
# 1. 基础设施 (Infrastructure)
# ==========================================
class GridWorldEnv:
    """
    环境模块：支持多陷阱的 GridWorld
    """

    def __init__(self):
        self.size = Config.GRID_SIZE
        self.state = (0, 0)
        self.goal = (3, 3)
        self.traps = Config.TRAPS  # 加载陷阱配置

    def reset(self) -> torch.Tensor:
        """重置环境至起点"""
        self.state = (0, 0)
        return self._to_tensor(self.state)

    def step(self, action: int) -> Tuple[torch.Tensor, float, bool, Tuple[int, int]]:
        """
        执行动作并返回反馈
        Returns: (next_state_tensor, reward, done, next_state_coord)
        """
        x, y = self.state

        # 0:Up, 1:Down, 2:Left, 3:Right
        # 注意：这里的 x 代表行(row), y 代表列(col)
        if action == 0:
            x = max(0, x - 1)
        elif action == 1:
            x = min(self.size - 1, x + 1)
        elif action == 2:
            y = max(0, y - 1)
        elif action == 3:
            y = min(self.size - 1, y + 1)

        next_state_coord = (x, y)
        self.state = next_state_coord

        # 默认步数惩罚 (鼓励最短路径)
        reward = -1.0
        done = False

        # 碰撞检测逻辑
        if next_state_coord == self.goal:
            reward = 10.0
            done = True
        elif next_state_coord in self.traps:  # 检查是否踩中任一陷阱
            reward = -10.0
            done = True

        return self._to_tensor(next_state_coord), reward, done, next_state_coord

    def _to_tensor(self, state_coord: Tuple[int, int]) -> torch.Tensor:
        """辅助函数：坐标 -> One-hot Tensor"""
        idx = state_coord[0] * self.size + state_coord[1]
        vec = np.zeros(Config.STATE_DIM, dtype=np.float32)
        vec[idx] = 1.0
        return torch.FloatTensor(vec).to(Config.DEVICE)


class ReplayBuffer:
    """
    数据层：经验回放池
    """

    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size) -> Tuple[torch.Tensor, ...]:
        """随机采样并自动移动到计算设备"""
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = zip(*batch)

        return (torch.stack(state),
                torch.LongTensor(action).to(Config.DEVICE),
                torch.FloatTensor(reward).to(Config.DEVICE),
                torch.stack(next_state),
                torch.FloatTensor(done).to(Config.DEVICE))

    def __len__(self):
        return len(self.buffer)


# ==========================================
# 2. 核心算法 (DQN Algorithm)
# ==========================================
class QNetwork(nn.Module):
    """模型层：简单的 3 层 MLP"""

    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(Config.STATE_DIM, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, Config.ACTION_DIM)
        )

    def forward(self, x):
        return self.net(x)


class DQNSolver:
    """
    业务逻辑层：DQN 训练器
    包含 Main Net, Target Net 和 Bellman Update 逻辑
    """

    def __init__(self):
        self.main_net = QNetwork().to(Config.DEVICE)
        self.target_net = QNetwork().to(Config.DEVICE)
        self.target_net.load_state_dict(self.main_net.state_dict())
        self.target_net.eval()  # Target Net 永远不需要计算梯度

        self.optimizer = optim.Adam(self.main_net.parameters(), lr=Config.LR)
        self.loss_fn = nn.MSELoss()

    def train_step(self, buffer: ReplayBuffer) -> float:
        """执行单步梯度下降"""
        if len(buffer) < Config.BATCH_SIZE: return 0.0

        # 1. 采样 (Uniformly draw samples)
        states, actions, rewards, next_states, dones = buffer.sample(Config.BATCH_SIZE)

        # 2. 计算当前 Q 值 (Main Network)
        # gather 用于选出我们实际采取的动作对应的 Q 值
        q_values = self.main_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        # 3. 计算目标 Q 值 (Target Network)
        # y_T = r + gamma * max Q(s', a')
        with torch.no_grad():
            next_q_max = self.target_net(next_states).max(1)[0]
            # 如果 done=1，则 (1-done)=0，目标值就是 reward 本身
            target_values = rewards + Config.GAMMA * next_q_max * (1 - dones)

        # 4. 更新网络 (Update Main Network)
        loss = self.loss_fn(q_values, target_values)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

    def sync_target(self):
        """同步参数 w_T = w"""
        self.target_net.load_state_dict(self.main_net.state_dict())


# ==========================================
# 3. 可视化模块 (Visualization)
# ==========================================
def visualize_policy(model):
    """
    绘制网格、陷阱及 Agent 的最优策略箭头
    """
    fig, ax = plt.subplots(figsize=(6, 6))
    ax.set_xlim(0, Config.GRID_SIZE)
    ax.set_ylim(0, Config.GRID_SIZE)
    ax.invert_yaxis()  # 翻转 Y 轴，使 (0,0) 在左上角

    # 1. 绘制网格线
    for i in range(Config.GRID_SIZE + 1):
        ax.plot([0, Config.GRID_SIZE], [i, i], color='black', alpha=0.3)
        ax.plot([i, i], [0, Config.GRID_SIZE], color='black', alpha=0.3)

    # 2. 绘制所有陷阱 (红色)
    for (r, c) in Config.TRAPS:
        ax.add_patch(patches.Rectangle((c, r), 1, 1, color='red', alpha=0.4))
        ax.text(c + 0.5, r + 0.5, 'TRAP', ha='center', va='center', fontweight='bold', fontsize=8, color='darkred')

    # 3. 绘制终点 (绿色)
    gx, gy = Config.GRID_SIZE - 1, Config.GRID_SIZE - 1
    ax.add_patch(patches.Rectangle((gy, gx), 1, 1, color='green', alpha=0.4))
    ax.text(gy + 0.5, gx + 0.5, 'GOAL', ha='center', va='center', fontweight='bold', color='darkgreen')

    # 4. 策略推断与绘制箭头
    model.eval()
    dx_dy = {0: (0, -0.3), 1: (0, 0.3), 2: (-0.3, 0), 3: (0.3, 0)}  # Up, Down, Left, Right

    for r in range(Config.GRID_SIZE):
        for c in range(Config.GRID_SIZE):
            # 跳过终点和陷阱，不画箭头
            if (r, c) == (3, 3) or (r, c) in Config.TRAPS:
                continue

            # 构造状态向量
            idx = r * Config.GRID_SIZE + c
            vec = np.zeros(Config.STATE_DIM, dtype=np.float32)
            vec[idx] = 1.0
            state_tensor = torch.FloatTensor(vec).to(Config.DEVICE)

            # 模型预测最佳动作
            with torch.no_grad():
                q_values = model(state_tensor)
                best_action = q_values.argmax().item()

            # 绘制箭头
            cx, cy = c + 0.5, r + 0.5
            dx, dy = dx_dy[best_action]
            ax.arrow(cx, cy, dx, dy, head_width=0.1, head_length=0.1, fc='blue', ec='blue')

    plt.title(f"DQN Learned Policy with {len(Config.TRAPS)} Traps")
    plt.show()


# ==========================================
# 4. 主程序入口 (Main Workflow)
# ==========================================
if __name__ == "__main__":
    # 初始化
    env = GridWorldEnv()
    buffer = ReplayBuffer(capacity=Config.COLLECTION_STEPS + 1000)
    solver = DQNSolver()

    logger.info("==========================================")
    logger.info(f"Start DQN Experiment | Device: {Config.DEVICE}")
    logger.info(f"Traps Location: {Config.TRAPS}")
    logger.info("==========================================")

    # --- Phase 1: Behavior Policy Collection (随机探索) ---
    logger.info("[Phase 1] Collecting random experience...")
    state = env.reset()
    for step in range(Config.COLLECTION_STEPS):
        # 纯随机动作 (Behavior Policy)
        action = random.randint(0, Config.ACTION_DIM - 1)

        next_state, reward, done, _ = env.step(action)
        buffer.push(state, action, reward, next_state, done)

        if done:
            state = env.reset()
        else:
            state = next_state

    logger.info(f"-> Collected {len(buffer)} samples.")

    # --- Phase 2: Off-Policy Training (离线训练) ---
    logger.info("[Phase 2] Training DQN off-policy...")
    for i in range(Config.TRAINING_ITERATIONS):
        loss = solver.train_step(buffer)

        # 定期同步 Target Network
        if (i + 1) % Config.TARGET_UPDATE_FREQ == 0:
            solver.sync_target()

        if (i + 1) % 500 == 0:
            logger.info(f"Iteration {i + 1}/{Config.TRAINING_ITERATIONS} | Loss: {loss:.5f}")

    # --- Phase 3: Visualization (策略展示) ---
    logger.info("[Phase 3] Visualizing Final Policy...")
    visualize_policy(solver.main_net)
